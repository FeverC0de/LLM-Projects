{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Web Scraper\n",
    "\n",
    "### Some goodies to import first for our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed to run the command below to setup our llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull llama3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "user_input = \"\"\n",
    "\n",
    "while user_input != \"exit\":\n",
    "    user_input = input(\"Type a prompt for the llm\")\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\" : \"doctor\", \"content\": f\"{user_input}\"}]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stuff I Learnt\n",
    "\n",
    "```python\n",
    "openai.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = [...]\n",
    ")\n",
    "```\n",
    "Generates a chat completion based on the model and the history. \"role\": defines who the speaker is\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Website Class\n",
    "\n",
    "Here we use the BeautifulSoup libary which is used for webscraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stuff I Learnt\n",
    "\n",
    "```python\n",
    "requests.get(url, headers=headers)\n",
    "```\n",
    "I come accross a familiar method requests.get. However what's different is that it takes in a \"headers\" argument. Why?\n",
    ".get Makes an HTTP request. In making an HTTP request, there needs to be info about the users device/web browser or it's mostly expected to have that information. Without it, it's pretty obvious this is a webscrapping bot. Hence we add it to bypass any anti scrapping measures\n",
    "\n",
    "```python\n",
    "    BeautifulSoup(response.content, 'html.parser')\n",
    "```\n",
    "\n",
    "The response.content gives us the raw HTML of the webpage. \n",
    "\n",
    "```python\n",
    "\n",
    "...\"html.parser\")\n",
    "```\n",
    "This converts raw HTML into a structured BeautifulSoup Object\n",
    "\n",
    "```python\n",
    "for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "    irrelevant.decompose()\n",
    "```\n",
    "Removes elements in HTML that we don't want. \n",
    "\n",
    "```soup.body()```\n",
    "Specifies the <body> tag in the HTML document\n",
    "The list has all the various tages we want to iterate over and irrelvant.decompose()\n",
    "\n",
    "```irrelevant.decompose()```\n",
    "Removes the element completely. There is a ```.extract() ``` method which detaches the element and keeps it in memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Prompts\n",
    "\n",
    "Prompts are necessary as they tell the model what to do.\n",
    "\n",
    "```system_prompt``` gives the model it's identity\n",
    "\n",
    "```user_prompt``` gives the users input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "    \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\":user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we set the messages for both the system and the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Webscrapping Time!\n",
    "This is a nice function to run everything when we pass in our url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"llama3.2\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested it on my ResNet50 project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Summary of Recognize Object Website\\n\\n## Purpose\\nRecognize Object is a platform that appears to be designed to identify objects through machine learning models. The website showcases the tool\\'s capabilities, particularly its use of Google\\'s QuickDraw dataset.\\n\\n### ResNet Drawing Guesser Tool\\nThe site describes a \"ResNet Drawing Guesser\" made from Google\\'s quickdraw dataset, intended for object recognition.\\n\\nNo news or announcements were present in the provided content.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"http://localhost:8000/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
